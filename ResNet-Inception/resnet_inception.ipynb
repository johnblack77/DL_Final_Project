{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import namedtuple\n",
    "from math import sqrt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_DIGITS = 10  # Number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def resnet_model_fn(features, labels, mode): \n",
    "    \n",
    "    with tf.device('/device:GPU:0'):\n",
    "        Group = namedtuple('Group', ['density', 'num_filters'])\n",
    "    \n",
    "        groups = [\n",
    "            Group(6, 32), Group(12, 32),\n",
    "            Group(24, 32), Group(16, 32)\n",
    "        ]\n",
    "    \n",
    "        #groups = [\n",
    "        #    Group(6, 64), Group(4, 64),\n",
    "        #    Group(4, 64), Group(4, 64)\n",
    "        #]\n",
    "    \n",
    "        input_layer = tf.reshape(features[\"x\"], [-1, 32, 32, 3])\n",
    "        input_layer = tf.map_fn(lambda frame: tf.image.per_image_standardization(frame), input_layer)\n",
    "\n",
    "        with tf.variable_scope('conv_layer1'):\n",
    "            bn = tf.layers.batch_normalization(inputs=input_layer, training=mode==tf.estimator.ModeKeys.TRAIN)\n",
    "            bn_relu = tf.nn.relu(bn)\n",
    "            conv = tf.layers.conv2d(\n",
    "                bn_relu,\n",
    "                filters=16,\n",
    "                kernel_size=3,\n",
    "                padding='same')\n",
    "            net = tf.layers.dropout(inputs=conv, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        #[100,32,32,16]\n",
    "\n",
    "        with tf.variable_scope('conv_layer2'):\n",
    "            net = tf.layers.conv2d(\n",
    "                net,\n",
    "                filters=groups[0].num_filters,\n",
    "                kernel_size=1,\n",
    "                padding='valid')\n",
    "\n",
    "        for group_i, group in enumerate(groups):\n",
    "            input_net = net\n",
    "            for layer_i in range(group.density):\n",
    "                name = 'group_%d/layer_%d' % (group_i, layer_i)\n",
    "                with tf.variable_scope(name + '/dense'):\n",
    "                    bn = tf.layers.batch_normalization(inputs=net, training=mode==tf.estimator.ModeKeys.TRAIN)\n",
    "                    bn_relu = tf.nn.relu(bn)\n",
    "                    conv = tf.layers.conv2d(\n",
    "                        bn_relu,\n",
    "                        filters=group.num_filters,\n",
    "                        kernel_size=3,\n",
    "                        padding='same')\n",
    "                    conv_dropout = tf.layers.dropout(inputs=conv, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "                    #net = tf.concat(axis=3, values=(net, conv_dropout)) \n",
    "                    net = net + conv_dropout\n",
    "            with tf.variable_scope('group_%d/conv_reduce' % group_i):    \n",
    "                bn = tf.layers.batch_normalization(inputs=net, training=mode==tf.estimator.ModeKeys.TRAIN)\n",
    "                bn_relu = tf.nn.relu(bn)\n",
    "                input_dim = int(input_net.get_shape()[-1])\n",
    "                conv = tf.layers.conv2d(\n",
    "                    bn_relu,\n",
    "                    filters=input_dim,\n",
    "                    kernel_size=1,\n",
    "                    padding='valid')\n",
    "                conv_dropout = tf.layers.dropout(inputs=conv, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "            net = conv_dropout + input_net\n",
    "            try:\n",
    "                next_group = groups[group_i + 1]\n",
    "                with tf.variable_scope('group_%d/conv_upscale' % group_i):\n",
    "                    net = tf.layers.conv2d(\n",
    "                        net,\n",
    "                        filters=next_group.num_filters,\n",
    "                        kernel_size=1,\n",
    "                        padding='same',\n",
    "                        activation=None,\n",
    "                        bias_initializer=None)\n",
    "            except IndexError:\n",
    "                pass\n",
    "        \n",
    "        net_shape = net.get_shape().as_list()\n",
    "        net = tf.nn.avg_pool(\n",
    "            net,\n",
    "            ksize=[1, net_shape[1], net_shape[2], 1],\n",
    "            strides=[1, 1, 1, 1],\n",
    "            padding='VALID')\n",
    "\n",
    "        net_shape = net.get_shape().as_list()\n",
    "        net = tf.reshape(net, [-1, net_shape[1] * net_shape[2] * net_shape[3]])\n",
    "  \n",
    "        logits = tf.layers.dense(net, N_DIGITS, activation=None)\n",
    "\n",
    "        predicted_classes = tf.argmax(logits, 1)\n",
    "        predictions = {\n",
    "                'classes': predicted_classes,\n",
    "                'probabilities': tf.nn.softmax(logits)\n",
    "        }\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "        onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), N_DIGITS, 1, 0)\n",
    "        loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:  \n",
    "            learning_rate_with_decay = tf.train.exponential_decay(learning_rate=0.1, global_step=tf.train.get_global_step(), decay_steps=10000, decay_rate=0.8, staircase=True)\n",
    "            optimizer = tf.train.AdagradOptimizer(learning_rate_with_decay)\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # batch norm\n",
    "            with tf.control_dependencies(extra_update_ops):\n",
    "                train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "            correct_predictions = tf.equal(predictions[\"classes\"], tf.argmax(onehot_labels, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "            tf.summary.scalar(\"Training_Accuracy\", accuracy)\n",
    "            \n",
    "            total_parameters = 0\n",
    "            for variable in tf.global_variables():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parametes = 1\n",
    "                for dim in shape:\n",
    "                    variable_parametes *= dim.value\n",
    "                total_parameters += variable_parametes\n",
    "            tf.summary.scalar(\"total_params\", total_parameters)\n",
    "                \n",
    "            tensors_log = {\n",
    "                \"loss\": loss,\n",
    "                \"train_accuracy\": accuracy\n",
    "            }\n",
    "            \n",
    "            logging_hook = tf.train.LoggingTensorHook(tensors=tensors_log, every_n_iter=1000)\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, training_hooks=[logging_hook])\n",
    "  \n",
    "        eval_metric_ops = {\n",
    "            'accuracy': tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predicted_classes)\n",
    "        }\n",
    "       \n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.contrib.keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = np.asarray(X_train, dtype=np.float32)\n",
    "y_train = np.asarray(y_train, dtype=np.int32).squeeze()\n",
    "X_test = np.asarray(X_test, dtype=np.float32)\n",
    "y_test = np.asarray(y_test, dtype=np.int32).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/Users/snehanagaraj/Documents/DL/Project/WDN/wide_dense_net_v6/cifar', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /Users/snehanagaraj/Documents/DL/Project/WDN/wide_dense_net_v6/cifar/model.ckpt-1\n",
      "INFO:tensorflow:Saving checkpoints for 2 into /Users/snehanagaraj/Documents/DL/Project/WDN/wide_dense_net_v6/cifar/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.82336, step = 2\n",
      "INFO:tensorflow:loss = 3.82336, train_accuracy = 0.17\n",
      "INFO:tensorflow:Loss for final step: 3.82336.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x152681b38>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = tf.estimator.Estimator(model_fn=resnet_model_fn, model_dir=\"/Users/snehanagaraj/Documents/DL/Project/WDN/wide_dense_net_v6/cifar\")\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)  # Show training logs.\n",
    "\n",
    "# Train model and save summaries into logdir.\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": X_train},\n",
    "    y=y_train,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "classifier.train(input_fn=train_input_fn, steps=1)\n",
    "\n",
    "# Calculate accuracy.\n",
    "#test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#    x={\"x\": X_test},\n",
    "#    y=y_test,\n",
    "#    num_epochs=1,\n",
    "#    shuffle=False)\n",
    "#scores = classifier.evaluate(input_fn=test_input_fn)\n",
    "#print('Accuracy: {0:f}'.format(scores['accuracy']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda env tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
